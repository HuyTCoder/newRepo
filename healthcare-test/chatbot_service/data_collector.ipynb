{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced WHO Medical Knowledge Extractor\n",
    "==========================================\n",
    "\n",
    "This advanced system transforms the basic scraper into a comprehensive medical knowledge\n",
    "processing engine with improved performance, intelligence, and reliability.\n",
    "\n",
    "Key Improvements:\n",
    "1. Asynchronous processing for better performance\n",
    "2. Advanced NLP for better content understanding\n",
    "3. Robust error handling with retry mechanisms\n",
    "4. Configuration management for flexibility\n",
    "5. Database integration for persistent storage\n",
    "6. Enhanced data validation and quality assurance\n",
    "7. Modular design for maintainability\n",
    "8. Advanced medical content classification\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import re\n",
    "import sqlite3\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import hashlib\n",
    "from contextlib import asynccontextmanager\n",
    "import time\n",
    "import yaml\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Enhanced imports for NLP and data processing\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    NLTK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"NLTK not available. Install with: pip install nltk\")\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup, NavigableString\n",
    "    BS4_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BS4_AVAILABLE = False\n",
    "    print(\"BeautifulSoup not available. Install with: pip install beautifulsoup4\")\n",
    "\n",
    "# Configure comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('medical_scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration Management\n",
    "@dataclass\n",
    "class ScrapingConfig:\n",
    "    \"\"\"Configuration class for scraping parameters - think of this as your control panel\"\"\"\n",
    "    max_concurrent_requests: int = 5  # How many pages to process simultaneously\n",
    "    request_delay: float = 2.0        # Respectful delay between requests\n",
    "    max_retries: int = 3              # How many times to retry failed requests\n",
    "    timeout: int = 30                 # Request timeout in seconds\n",
    "    max_content_length: int = 1000000 # Maximum page size to process\n",
    "    enable_nlp: bool = True           # Whether to use advanced text processing\n",
    "    cache_duration: int = 24          # Cache validity in hours\n",
    "    output_formats: List[str] = None  # Supported output formats\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.output_formats is None:\n",
    "            self.output_formats = ['json', 'sqlite', 'csv']\n",
    "\n",
    "# Data Models for Better Structure\n",
    "@dataclass\n",
    "class MedicalEntity:\n",
    "    \"\"\"Represents a single piece of medical information with metadata\"\"\"\n",
    "    content: str\n",
    "    confidence: float        # How confident we are in this extraction (0-1)\n",
    "    source_section: str     # Which section this came from\n",
    "    extraction_method: str  # How this was extracted (keywords, nlp, etc.)\n",
    "    \n",
    "@dataclass\n",
    "class DiseaseInformation:\n",
    "    \"\"\"Comprehensive disease information model - like a medical record\"\"\"\n",
    "    name: str\n",
    "    overview: str\n",
    "    symptoms: List[MedicalEntity]\n",
    "    risk_factors: List[MedicalEntity]\n",
    "    prevention: List[MedicalEntity]\n",
    "    diagnosis: List[MedicalEntity]\n",
    "    treatment: List[MedicalEntity]\n",
    "    statistics: Dict[str, Any]\n",
    "    severity: str\n",
    "    prevalence: float\n",
    "    key_facts: List[str]\n",
    "    source_metadata: Dict[str, Any]\n",
    "    last_updated: datetime\n",
    "    data_quality_score: float  # Overall quality assessment (0-1)\n",
    "\n",
    "# Advanced Content Processor using NLP\n",
    "class MedicalContentProcessor:\n",
    "    \"\"\"\n",
    "    Advanced text processing engine that understands medical content better.\n",
    "    Think of this as a medical student who's learned to identify different\n",
    "    types of medical information more accurately.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, enable_nlp: bool = True):\n",
    "        self.enable_nlp = enable_nlp\n",
    "        self.medical_keywords = self._load_medical_keywords()\n",
    "        \n",
    "        if enable_nlp and NLTK_AVAILABLE:\n",
    "            self._initialize_nltk()\n",
    "    \n",
    "    def _initialize_nltk(self):\n",
    "        \"\"\"Download required NLTK data if not present\"\"\"\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('wordnet')\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def _load_medical_keywords(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Load comprehensive medical keyword dictionaries.\n",
    "        This is like giving our system a medical vocabulary.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'symptoms': [\n",
    "                'symptom', 'sign', 'manifestation', 'present', 'experience', 'feel',\n",
    "                'pain', 'ache', 'fever', 'headache', 'fatigue', 'weakness', 'nausea',\n",
    "                'vomiting', 'diarrhea', 'cough', 'shortness of breath', 'chest pain',\n",
    "                'swelling', 'inflammation', 'rash', 'itching', 'dizziness', 'confusion'\n",
    "            ],\n",
    "            'risk_factors': [\n",
    "                'risk factor', 'risk', 'factor', 'cause', 'associated', 'increase',\n",
    "                'likely', 'predispose', 'contribute', 'age', 'gender', 'genetics',\n",
    "                'lifestyle', 'smoking', 'alcohol', 'obesity', 'diabetes', 'hypertension'\n",
    "            ],\n",
    "            'prevention': [\n",
    "                'prevent', 'prevention', 'avoid', 'reduce', 'lifestyle', 'diet',\n",
    "                'exercise', 'vaccination', 'immunization', 'screening', 'early detection',\n",
    "                'healthy eating', 'physical activity', 'weight management', 'quit smoking'\n",
    "            ],\n",
    "            'diagnosis': [\n",
    "                'diagnosis', 'diagnostic', 'test', 'testing', 'screen', 'screening',\n",
    "                'detect', 'detection', 'measure', 'measurement', 'examine', 'examination',\n",
    "                'blood test', 'imaging', 'x-ray', 'ct scan', 'mri', 'biopsy', 'laboratory'\n",
    "            ],\n",
    "            'treatment': [\n",
    "                'treatment', 'treat', 'therapy', 'therapeutic', 'manage', 'management',\n",
    "                'medication', 'medicine', 'drug', 'surgery', 'surgical', 'intervention',\n",
    "                'procedure', 'rehabilitation', 'care', 'antibiotic', 'antiviral'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def extract_medical_entities(self, text: str, section_title: str = \"\") -> List[MedicalEntity]:\n",
    "        \"\"\"\n",
    "        Extract medical entities with confidence scores.\n",
    "        This method acts like a medical expert reading through text and\n",
    "        identifying important medical information with varying degrees of certainty.\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        entities = []\n",
    "        \n",
    "        # First, try keyword-based extraction (fast and reliable)\n",
    "        keyword_entities = self._extract_by_keywords(text, section_title)\n",
    "        entities.extend(keyword_entities)\n",
    "        \n",
    "        # Then, use NLP for more sophisticated extraction if available\n",
    "        if self.enable_nlp and NLTK_AVAILABLE:\n",
    "            nlp_entities = self._extract_by_nlp(text, section_title)\n",
    "            entities.extend(nlp_entities)\n",
    "        \n",
    "        # Remove duplicates and merge similar content\n",
    "        entities = self._deduplicate_entities(entities)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _extract_by_keywords(self, text: str, section_title: str) -> List[MedicalEntity]:\n",
    "        \"\"\"Extract entities using keyword matching - like a medical keyword dictionary\"\"\"\n",
    "        entities = []\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_lower = sentence.lower()\n",
    "            \n",
    "            # Determine category based on keywords and section title\n",
    "            category = self._classify_sentence(sentence_lower, section_title)\n",
    "            if not category:\n",
    "                continue\n",
    "            \n",
    "            # Calculate confidence based on keyword density and sentence quality\n",
    "            confidence = self._calculate_keyword_confidence(sentence_lower, category)\n",
    "            \n",
    "            if confidence > 0.3:  # Only include reasonably confident extractions\n",
    "                entities.append(MedicalEntity(\n",
    "                    content=sentence.strip(),\n",
    "                    confidence=confidence,\n",
    "                    source_section=section_title,\n",
    "                    extraction_method=\"keyword_matching\"\n",
    "                ))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _extract_by_nlp(self, text: str, section_title: str) -> List[MedicalEntity]:\n",
    "        \"\"\"\n",
    "        Use NLP techniques for more sophisticated extraction.\n",
    "        This is like having a medical student who understands grammar and context.\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        try:\n",
    "            # Tokenize into sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                # Skip very short or very long sentences\n",
    "                if len(sentence.split()) < 5 or len(sentence.split()) > 50:\n",
    "                    continue\n",
    "                \n",
    "                # Analyze sentence structure and medical content\n",
    "                features = self._extract_sentence_features(sentence)\n",
    "                category = self._classify_by_features(features, section_title)\n",
    "                \n",
    "                if category:\n",
    "                    confidence = self._calculate_nlp_confidence(features, category)\n",
    "                    \n",
    "                    if confidence > 0.4:  # Higher threshold for NLP extraction\n",
    "                        entities.append(MedicalEntity(\n",
    "                            content=sentence.strip(),\n",
    "                            confidence=confidence,\n",
    "                            source_section=section_title,\n",
    "                            extraction_method=\"nlp_analysis\"\n",
    "                        ))\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"NLP extraction failed: {e}\")\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Smart sentence splitting that handles medical text peculiarities\"\"\"\n",
    "        # Handle common medical abbreviations that contain periods\n",
    "        text = re.sub(r'\\b(Dr|Mr|Mrs|Ms|Ph\\.D|M\\.D|etc)\\.', r'\\1<PERIOD>', text)\n",
    "        \n",
    "        # Split by sentence endings\n",
    "        sentences = re.split(r'[.!?]+\\s+', text)\n",
    "        \n",
    "        # Restore abbreviations\n",
    "        sentences = [s.replace('<PERIOD>', '.') for s in sentences]\n",
    "        \n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    def _classify_sentence(self, sentence: str, section_title: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Classify a sentence into medical categories.\n",
    "        This works like a medical student learning to categorize information.\n",
    "        \"\"\"\n",
    "        section_lower = section_title.lower()\n",
    "        \n",
    "        # First, check if section title gives us a strong hint\n",
    "        for category, keywords in self.medical_keywords.items():\n",
    "            if any(keyword in section_lower for keyword in keywords[:3]):  # Check main keywords\n",
    "                return category\n",
    "        \n",
    "        # Then, analyze sentence content\n",
    "        best_category = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for category, keywords in self.medical_keywords.items():\n",
    "            score = sum(1 for keyword in keywords if keyword in sentence)\n",
    "            if score > best_score and score >= 2:  # Need at least 2 matching keywords\n",
    "                best_score = score\n",
    "                best_category = category\n",
    "        \n",
    "        return best_category\n",
    "    \n",
    "    def _calculate_keyword_confidence(self, sentence: str, category: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate confidence based on keyword matching.\n",
    "        More medical keywords = higher confidence, like a medical expert\n",
    "        being more certain when they see familiar patterns.\n",
    "        \"\"\"\n",
    "        keywords = self.medical_keywords.get(category, [])\n",
    "        matches = sum(1 for keyword in keywords if keyword in sentence)\n",
    "        \n",
    "        # Base confidence from keyword density\n",
    "        keyword_confidence = min(matches / 3.0, 1.0)  # Max at 3 keywords\n",
    "        \n",
    "        # Adjust based on sentence quality indicators\n",
    "        quality_factors = {\n",
    "            'appropriate_length': 0.8 if 10 <= len(sentence.split()) <= 30 else 0.5,\n",
    "            'has_medical_terms': 0.9 if any(term in sentence for term in \n",
    "                ['patient', 'treatment', 'diagnosis', 'symptom', 'disease']) else 0.7,\n",
    "            'not_too_general': 0.8 if len(sentence) > 20 else 0.5\n",
    "        }\n",
    "        \n",
    "        quality_score = sum(quality_factors.values()) / len(quality_factors)\n",
    "        \n",
    "        return keyword_confidence * quality_score\n",
    "    \n",
    "    def _extract_sentence_features(self, sentence: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract linguistic and medical features from a sentence\"\"\"\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        \n",
    "        return {\n",
    "            'word_count': len(words),\n",
    "            'medical_word_ratio': sum(1 for word in words if self._is_medical_word(word)) / len(words),\n",
    "            'has_numbers': bool(re.search(r'\\d+', sentence)),\n",
    "            'has_percentages': bool(re.search(r'\\d+%', sentence)),\n",
    "            'sentence_complexity': len([w for w in words if w not in self.stop_words]) / len(words),\n",
    "            'contains_action_words': any(word in sentence.lower() for word in \n",
    "                ['cause', 'prevent', 'treat', 'diagnose', 'manage']),\n",
    "            'medical_entity_count': len(re.findall(r'\\b(?:mg|ml|mmHg|bpm|temperature|blood pressure)\\b', sentence.lower()))\n",
    "        }\n",
    "    \n",
    "    def _is_medical_word(self, word: str) -> bool:\n",
    "        \"\"\"Check if a word is likely medical terminology\"\"\"\n",
    "        medical_suffixes = ['itis', 'osis', 'emia', 'pathy', 'therapy', 'gram', 'scopy']\n",
    "        medical_prefixes = ['anti', 'hyper', 'hypo', 'pre', 'post', 'inter', 'intra']\n",
    "        \n",
    "        return (word in [item for sublist in self.medical_keywords.values() for item in sublist] or\n",
    "                any(word.endswith(suffix) for suffix in medical_suffixes) or\n",
    "                any(word.startswith(prefix) for prefix in medical_prefixes))\n",
    "    \n",
    "    def _classify_by_features(self, features: Dict[str, Any], section_title: str) -> Optional[str]:\n",
    "        \"\"\"Classify based on extracted features using simple rules\"\"\"\n",
    "        # This is a simplified classifier - in production, you might use machine learning\n",
    "        \n",
    "        if 'symptom' in section_title.lower() or 'sign' in section_title.lower():\n",
    "            return 'symptoms'\n",
    "        elif 'risk' in section_title.lower() or 'factor' in section_title.lower():\n",
    "            return 'risk_factors'\n",
    "        elif 'prevent' in section_title.lower():\n",
    "            return 'prevention'\n",
    "        elif 'diagnos' in section_title.lower() or 'test' in section_title.lower():\n",
    "            return 'diagnosis'\n",
    "        elif 'treat' in section_title.lower() or 'therap' in section_title.lower():\n",
    "            return 'treatment'\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _calculate_nlp_confidence(self, features: Dict[str, Any], category: str) -> float:\n",
    "        \"\"\"Calculate confidence based on NLP features\"\"\"\n",
    "        base_confidence = 0.5\n",
    "        \n",
    "        # Adjust based on features\n",
    "        if features['medical_word_ratio'] > 0.2:\n",
    "            base_confidence += 0.2\n",
    "        if features['has_numbers']:\n",
    "            base_confidence += 0.1\n",
    "        if features['contains_action_words']:\n",
    "            base_confidence += 0.1\n",
    "        if 5 <= features['word_count'] <= 25:\n",
    "            base_confidence += 0.1\n",
    "        \n",
    "        return min(base_confidence, 1.0)\n",
    "    \n",
    "    def _deduplicate_entities(self, entities: List[MedicalEntity]) -> List[MedicalEntity]:\n",
    "        \"\"\"\n",
    "        Remove duplicate and very similar entities.\n",
    "        This is like an editor reviewing medical notes and removing redundant information.\n",
    "        \"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        unique_entities = []\n",
    "        seen_content = set()\n",
    "        \n",
    "        # Sort by confidence (highest first)\n",
    "        entities.sort(key=lambda x: x.confidence, reverse=True)\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Simple deduplication based on content similarity\n",
    "            content_hash = hashlib.md5(entity.content.lower().encode()).hexdigest()\n",
    "            \n",
    "            if content_hash not in seen_content:\n",
    "                seen_content.add(content_hash)\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities\n",
    "\n",
    "# Enhanced Database Manager\n",
    "class MedicalDataManager:\n",
    "    \"\"\"\n",
    "    Manages persistent storage of medical data.\n",
    "    Think of this as a medical records system that keeps track of\n",
    "    all the diseases and their information over time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"medical_knowledge.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize database schema\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Main diseases table\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS diseases (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    name TEXT UNIQUE NOT NULL,\n",
    "                    overview TEXT,\n",
    "                    severity TEXT,\n",
    "                    prevalence REAL,\n",
    "                    data_quality_score REAL,\n",
    "                    source_url TEXT,\n",
    "                    last_updated TIMESTAMP,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Medical entities table (symptoms, treatments, etc.)\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS medical_entities (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    disease_id INTEGER,\n",
    "                    entity_type TEXT,  -- symptoms, risk_factors, etc.\n",
    "                    content TEXT,\n",
    "                    confidence REAL,\n",
    "                    source_section TEXT,\n",
    "                    extraction_method TEXT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    FOREIGN KEY (disease_id) REFERENCES diseases (id)\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Statistics table\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS disease_statistics (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    disease_id INTEGER,\n",
    "                    stat_type TEXT,\n",
    "                    stat_value TEXT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    FOREIGN KEY (disease_id) REFERENCES diseases (id)\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            conn.commit()\n",
    "    \n",
    "    def save_disease(self, disease_info: DiseaseInformation) -> int:\n",
    "        \"\"\"Save disease information to database\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Insert or update main disease record\n",
    "            cursor.execute('''\n",
    "                INSERT OR REPLACE INTO diseases \n",
    "                (name, overview, severity, prevalence, data_quality_score, source_url, last_updated)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                disease_info.name,\n",
    "                disease_info.overview,\n",
    "                disease_info.severity,\n",
    "                disease_info.prevalence,\n",
    "                disease_info.data_quality_score,\n",
    "                disease_info.source_metadata.get('source_url'),\n",
    "                disease_info.last_updated\n",
    "            ))\n",
    "            \n",
    "            disease_id = cursor.lastrowid\n",
    "            \n",
    "            # Clear existing entities for this disease\n",
    "            cursor.execute('DELETE FROM medical_entities WHERE disease_id = ?', (disease_id,))\n",
    "            cursor.execute('DELETE FROM disease_statistics WHERE disease_id = ?', (disease_id,))\n",
    "            \n",
    "            # Insert medical entities\n",
    "            entity_types = ['symptoms', 'risk_factors', 'prevention', 'diagnosis', 'treatment']\n",
    "            for entity_type in entity_types:\n",
    "                entities = getattr(disease_info, entity_type, [])\n",
    "                for entity in entities:\n",
    "                    cursor.execute('''\n",
    "                        INSERT INTO medical_entities \n",
    "                        (disease_id, entity_type, content, confidence, source_section, extraction_method)\n",
    "                        VALUES (?, ?, ?, ?, ?, ?)\n",
    "                    ''', (\n",
    "                        disease_id, entity_type, entity.content, entity.confidence,\n",
    "                        entity.source_section, entity.extraction_method\n",
    "                    ))\n",
    "            \n",
    "            # Insert statistics\n",
    "            for stat_type, stat_values in disease_info.statistics.items():\n",
    "                if isinstance(stat_values, list):\n",
    "                    for value in stat_values:\n",
    "                        cursor.execute('''\n",
    "                            INSERT INTO disease_statistics (disease_id, stat_type, stat_value)\n",
    "                            VALUES (?, ?, ?)\n",
    "                        ''', (disease_id, stat_type, str(value)))\n",
    "            \n",
    "            conn.commit()\n",
    "            return disease_id\n",
    "    \n",
    "    def get_disease_by_name(self, name: str) -> Optional[DiseaseInformation]:\n",
    "        \"\"\"Retrieve disease information by name\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.row_factory = sqlite3.Row\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Get main disease info\n",
    "            cursor.execute('SELECT * FROM diseases WHERE name = ?', (name,))\n",
    "            disease_row = cursor.fetchone()\n",
    "            \n",
    "            if not disease_row:\n",
    "                return None\n",
    "            \n",
    "            # Get medical entities\n",
    "            entities_by_type = {}\n",
    "            for entity_type in ['symptoms', 'risk_factors', 'prevention', 'diagnosis', 'treatment']:\n",
    "                cursor.execute('''\n",
    "                    SELECT * FROM medical_entities \n",
    "                    WHERE disease_id = ? AND entity_type = ?\n",
    "                ''', (disease_row['id'], entity_type))\n",
    "                \n",
    "                entities = []\n",
    "                for row in cursor.fetchall():\n",
    "                    entities.append(MedicalEntity(\n",
    "                        content=row['content'],\n",
    "                        confidence=row['confidence'],\n",
    "                        source_section=row['source_section'],\n",
    "                        extraction_method=row['extraction_method']\n",
    "                    ))\n",
    "                entities_by_type[entity_type] = entities\n",
    "            \n",
    "            # Get statistics\n",
    "            cursor.execute('SELECT * FROM disease_statistics WHERE disease_id = ?', (disease_row['id'],))\n",
    "            statistics = {}\n",
    "            for row in cursor.fetchall():\n",
    "                stat_type = row['stat_type']\n",
    "                if stat_type not in statistics:\n",
    "                    statistics[stat_type] = []\n",
    "                statistics[stat_type].append(row['stat_value'])\n",
    "            \n",
    "            return DiseaseInformation(\n",
    "                name=disease_row['name'],\n",
    "                overview=disease_row['overview'] or '',\n",
    "                symptoms=entities_by_type.get('symptoms', []),\n",
    "                risk_factors=entities_by_type.get('risk_factors', []),\n",
    "                prevention=entities_by_type.get('prevention', []),\n",
    "                diagnosis=entities_by_type.get('diagnosis', []),\n",
    "                treatment=entities_by_type.get('treatment', []),\n",
    "                statistics=statistics,\n",
    "                severity=disease_row['severity'],\n",
    "                prevalence=disease_row['prevalence'],\n",
    "                key_facts=[],  # Would need separate table for this\n",
    "                source_metadata={'source_url': disease_row['source_url']},\n",
    "                last_updated=datetime.fromisoformat(disease_row['last_updated']) if disease_row['last_updated'] else datetime.now(),\n",
    "                data_quality_score=disease_row['data_quality_score']\n",
    "            )\n",
    "\n",
    "# Enhanced Asynchronous Scraper\n",
    "class EnhancedWHOScraper:\n",
    "    \"\"\"\n",
    "    The main scraping engine with advanced capabilities.\n",
    "    This is like upgrading from a single medical researcher to a whole\n",
    "    team of researchers working efficiently together.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ScrapingConfig = None):\n",
    "        self.config = config or ScrapingConfig()\n",
    "        self.content_processor = MedicalContentProcessor(self.config.enable_nlp)\n",
    "        self.data_manager = MedicalDataManager()\n",
    "        self.session = None\n",
    "        self.processed_urls = set()\n",
    "        \n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Async context manager entry - sets up the HTTP session\"\"\"\n",
    "        connector = aiohttp.TCPConnector(limit=self.config.max_concurrent_requests)\n",
    "        timeout = aiohttp.ClientTimeout(total=self.config.timeout)\n",
    "        \n",
    "        self.session = aiohttp.ClientSession(\n",
    "            connector=connector,\n",
    "            timeout=timeout,\n",
    "            headers={\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Async context manager exit - cleans up the HTTP session\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "    \n",
    "    async def scrape_urls(self, urls: List[str]) -> Dict[str, DiseaseInformation]:\n",
    "        \"\"\"\n",
    "        Scrape multiple URLs concurrently with intelligent rate limiting.\n",
    "        This is like coordinating a team of researchers to work on different\n",
    "        diseases simultaneously while being respectful to the WHO servers.\n",
    "        \"\"\"\n",
    "        semaphore = asyncio.Semaphore(self.config.max_concurrent_requests)\n",
    "        results = {}\n",
    "        \n",
    "        # Create tasks for all URLs\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            if url not in self.processed_urls:\n",
    "                task = self._scrape_with_semaphore(semaphore, url)\n",
    "                tasks.append((url, task))\n",
    "        \n",
    "        # Process tasks and collect results\n",
    "        for url, task in tasks:\n",
    "            try:\n",
    "                disease_info = await task\n",
    "                if disease_info:\n",
    "                    results[disease_info.name] = disease_info\n",
    "                    # Save to database immediately\n",
    "                    self.data_manager.save_disease(disease_info)\n",
    "                    logger.info(f\"Successfully processed: {disease_info.name}\")\n",
    "                self.processed_urls.add(url)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process {url}: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def _scrape_with_semaphore(self, semaphore: asyncio.Semaphore, url: str) -> Optional[DiseaseInformation]:\n",
    "        \"\"\"Scrape a single URL with rate limiting\"\"\"\n",
    "        async with semaphore:\n",
    "            # Add respectful delay\n",
    "            await asyncio.sleep(self.config.request_delay)\n",
    "            return await self._scrape_single_url(url)\n",
    "    \n",
    "    async def _scrape_single_url(self, url: str) -> Optional[DiseaseInformation]:\n",
    "        \"\"\"\n",
    "        Scrape a single URL with retry logic and comprehensive error handling.\n",
    "        This method embodies the resilience of a dedicated researcher who\n",
    "        doesn't give up easily when faced with technical difficulties.\n",
    "        \"\"\"\n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                logger.info(f\"Scraping attempt {attempt + 1} for: {url}\")\n",
    "                \n",
    "                async with self.session.get(url) as response:\n",
    "                    if response.status != 200:\n",
    "                        logger.warning(f\"HTTP {response.status} for {url}\")\n",
    "                        continue\n",
    "                    \n",
    "                    content = await response.text()\n",
    "                    \n",
    "                    if len(content) > self.config.max_content_length:\n",
    "                        logger.warning(f\"Content too large for {url}, truncating\")\n",
    "                        content = content[:self.config.max_content_length]\n",
    "                    \n",
    "                    return await self._process_content(content, url)\n",
    "            \n",
    "            except asyncio.TimeoutError:\n",
    "                logger.warning(f\"Timeout for {url}, attempt {attempt + 1}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping {url}, attempt {attempt + 1}: {e}\")\n",
    "            \n",
    "            if attempt < self.config.max_retries - 1:\n",
    "                # Exponential backoff\n",
    "                await asyncio.sleep(2 ** attempt)\n",
    "        \n",
    "        logger.error(f\"Failed to scrape {url} after {self.config.max_retries} attempts\")\n",
    "        return None\n",
    "    \n",
    "    async def _process_content(self, content: str, url: str) -> Optional[DiseaseInformation]:\n",
    "        \"\"\"\n",
    "        Process HTML content into structured disease information.\n",
    "        This is the heart of our intelligent extraction system.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not BS4_AVAILABLE:\n",
    "                logger.error(\"BeautifulSoup not available for content processing\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # Extract basic information\n",
    "            disease_name = self._extract_disease_name(soup)\n",
    "            overview = self._extract_overview(soup)\n",
    "            \n",
    "            if not disease_name:\n",
    "                logger.warning(f\"Could not extract disease name from {url}\")\n",
    "                return None\n",
    "            \n",
    "            # Extract content sections with better structure detection\n",
    "            sections = self._extract_content_sections(soup)\n",
    "            \n",
    "            # Process each section with our advanced content processor\n",
    "            medical_entities = {}\n",
    "            for section_title, section_content in sections.items():\n",
    "                entities = self.content_processor.extract_medical_entities(\n",
    "                    section_content, section_title\n",
    "                )\n",
    "                \n",
    "                # Group entities by type\n",
    "                for entity in entities:\n",
    "                    category = self._determine_entity_category(entity, section_title)\n",
    "                    if category not in medical_entities:\n",
    "                        medical_entities[category] = []\n",
    "                    medical_entities[category].append(entity)\n",
    "            \n",
    "            # Extract statistics and metadata\n",
    "            statistics = self._extract_statistics(soup)\n",
    "            severity = self._determine_severity(soup, medical_entities)\n",
    "            prevalence = self._calculate_prevalence(statistics, disease_name)\n",
    "            \n",
    "            # Calculate overall data quality score\n",
    "            quality_score = self._calculate_quality_score(\n",
    "                medical_entities, statistics, overview, sections\n",
    "            )\n",
    "            \n",
    "            return DiseaseInformation(\n",
    "                name=disease_name,\n",
    "                overview=overview,\n",
    "                symptoms=medical_entities.get('symptoms', []),\n",
    "                risk_factors=medical_entities.get('risk_factors', []),\n",
    "                prevention=medical_entities.get('prevention', []),\n",
    "                diagnosis=medical_entities.get('diagnosis', []),\n",
    "                treatment=medical_entities.get('treatment', []),\n",
    "                statistics=statistics,\n",
    "                severity=severity,\n",
    "                prevalence=prevalence,\n",
    "                key_facts=self._extract_key_facts(soup),\n",
    "                source_metadata={\n",
    "                    'source_url': url,\n",
    "                    'scraped_date': datetime.now(),\n",
    "                    'source': 'World Health Organization (WHO)',\n",
    "                    'processing_version': '2.0'\n",
    "                },\n",
    "                last_updated=datetime.now(),\n",
    "                data_quality_score=quality_score\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing content from {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_disease_name(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract disease name with improved accuracy\"\"\"\n",
    "        # Try multiple methods to find the disease name\n",
    "        candidates = []\n",
    "        \n",
    "        # Method 1: Main heading\n",
    "        h1 = soup.find('h1')\n",
    "        if h1:\n",
    "            candidates.append(h1.get_text().strip())\n",
    "        \n",
    "        # Method 2: Title tag\n",
    "        title = soup.find('title')\n",
    "        if title:\n",
    "            title_text = title.get_text().strip()\n",
    "            # Remove common WHO prefixes/suffixes\n",
    "            title_text = re.sub(r'.*WHO.*?[-–]', '', title_text).strip()\n",
    "            candidates.append(title_text)\n",
    "        \n",
    "        # Method 3: Look for fact sheet patterns\n",
    "        fact_sheet_pattern = re.compile(r'fact sheet.*?[-–]\\s*(.+)', re.IGNORECASE)\n",
    "        for candidate in candidates:\n",
    "            match = fact_sheet_pattern.search(candidate)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "        \n",
    "        # Return the first non-empty candidate\n",
    "        for candidate in candidates:\n",
    "            if candidate and len(candidate) > 3:\n",
    "                return candidate\n",
    "        \n",
    "        return \"Unknown Disease\"\n",
    "    \n",
    "    def _extract_overview(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract disease overview with better content detection\"\"\"\n",
    "        # Look for common overview patterns\n",
    "        overview_indicators = ['overview', 'introduction', 'about', 'definition']\n",
    "        \n",
    "        for indicator in overview_indicators:\n",
    "            heading = soup.find(['h2', 'h3'], string=re.compile(indicator, re.I))\n",
    "            if heading:\n",
    "                # Get content after this heading\n",
    "                content = []\n",
    "                sibling = heading.next_sibling\n",
    "                while sibling and not (hasattr(sibling, 'name') and sibling.name in ['h1', 'h2', 'h3']):\n",
    "                    if hasattr(sibling, 'get_text'):\n",
    "                        text = sibling.get_text().strip()\n",
    "                        if text and len(text) > 20:\n",
    "                            content.append(text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                \n",
    "                if content:\n",
    "                    return ' '.join(content)[:500]  # Limit overview length\n",
    "        \n",
    "        # Fallback: first substantial paragraph\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for p in paragraphs[:3]:  # Check first 3 paragraphs\n",
    "            text = p.get_text().strip()\n",
    "            if len(text) > 100:  # Substantial content\n",
    "                return text[:500]\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def _extract_content_sections(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
    "        \"\"\"Enhanced section extraction with better content grouping\"\"\"\n",
    "        sections = {}\n",
    "        \n",
    "        # Find all headings and group content\n",
    "        headings = soup.find_all(['h2', 'h3', 'h4'])\n",
    "        \n",
    "        for i, heading in enumerate(headings):\n",
    "            heading_text = heading.get_text().strip()\n",
    "            if not heading_text:\n",
    "                continue\n",
    "            \n",
    "            # Collect content until next heading of same or higher level\n",
    "            content_parts = []\n",
    "            current = heading.next_sibling\n",
    "            \n",
    "            while current:\n",
    "                # Stop at next heading of same or higher level\n",
    "                if (hasattr(current, 'name') and \n",
    "                    current.name in ['h1', 'h2', 'h3', 'h4'] and\n",
    "                    int(current.name[1]) <= int(heading.name[1])):\n",
    "                    break\n",
    "                \n",
    "                if hasattr(current, 'get_text'):\n",
    "                    text = current.get_text().strip()\n",
    "                    if text and len(text) > 10:  # Filter out very short content\n",
    "                        content_parts.append(text)\n",
    "                \n",
    "                current = current.next_sibling\n",
    "            \n",
    "            if content_parts:\n",
    "                sections[heading_text] = ' '.join(content_parts)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _determine_entity_category(self, entity: MedicalEntity, section_title: str) -> str:\n",
    "        \"\"\"Determine the category of a medical entity with improved logic\"\"\"\n",
    "        section_lower = section_title.lower()\n",
    "        \n",
    "        # Use section title as primary indicator\n",
    "        if any(word in section_lower for word in ['symptom', 'sign', 'manifest']):\n",
    "            return 'symptoms'\n",
    "        elif any(word in section_lower for word in ['risk', 'factor', 'cause']):\n",
    "            return 'risk_factors'\n",
    "        elif any(word in section_lower for word in ['prevent', 'avoid']):\n",
    "            return 'prevention'\n",
    "        elif any(word in section_lower for word in ['diagnos', 'test', 'detect']):\n",
    "            return 'diagnosis'\n",
    "        elif any(word in section_lower for word in ['treat', 'therap', 'manage']):\n",
    "            return 'treatment'\n",
    "        \n",
    "        # Fallback to content analysis\n",
    "        content_lower = entity.content.lower()\n",
    "        if any(word in content_lower for word in ['experience', 'feel', 'pain', 'ache']):\n",
    "            return 'symptoms'\n",
    "        elif any(word in content_lower for word in ['increase risk', 'more likely']):\n",
    "            return 'risk_factors'\n",
    "        elif any(word in content_lower for word in ['prevent', 'avoid', 'reduce risk']):\n",
    "            return 'prevention'\n",
    "        elif any(word in content_lower for word in ['test', 'diagnose', 'detect']):\n",
    "            return 'diagnosis'\n",
    "        elif any(word in content_lower for word in ['treat', 'medication', 'therapy']):\n",
    "            return 'treatment'\n",
    "        \n",
    "        return 'general'  # Default category\n",
    "    \n",
    "    def _extract_statistics(self, soup: BeautifulSoup) -> Dict[str, List[str]]:\n",
    "        \"\"\"Enhanced statistics extraction with better pattern recognition\"\"\"\n",
    "        statistics = {}\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Enhanced patterns for different types of statistics\n",
    "        patterns = {\n",
    "            'deaths': r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:million|thousand)?\\s*(?:people\\s*)?(?:die|death|mortality)',\n",
    "            'affected': r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:million|billion|thousand)?\\s*(?:people\\s*)?(?:affected|living with|have)',\n",
    "            'percentage': r'(\\d+(?:\\.\\d+)?)\\s*%',\n",
    "            'prevalence': r'prevalence.*?(\\d+(?:\\.\\d+)?\\s*%)',\n",
    "            'incidence': r'incidence.*?(\\d+(?:,\\d{3})*)',\n",
    "            'cost': r'\\$(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million|thousand)?'\n",
    "        }\n",
    "        \n",
    "        for stat_type, pattern in patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                statistics[stat_type] = matches[:5]  # Limit to 5 matches per type\n",
    "        \n",
    "        return statistics\n",
    "    \n",
    "    def _determine_severity(self, soup: BeautifulSoup, medical_entities: Dict[str, List[MedicalEntity]]) -> str:\n",
    "        \"\"\"Enhanced severity determination using multiple indicators\"\"\"\n",
    "        content = soup.get_text().lower()\n",
    "        \n",
    "        # High severity indicators with weights\n",
    "        high_indicators = {\n",
    "            'death': 3, 'fatal': 3, 'mortality': 2, 'life-threatening': 3,\n",
    "            'emergency': 2, 'critical': 2, 'severe': 2, 'cancer': 2\n",
    "        }\n",
    "        \n",
    "        # Moderate severity indicators\n",
    "        moderate_indicators = {\n",
    "            'chronic': 1, 'manage': 1, 'control': 1, 'treatment': 1,\n",
    "            'medication': 1, 'therapy': 1, 'hospital': 2\n",
    "        }\n",
    "        \n",
    "        # Calculate severity score\n",
    "        high_score = sum(weight for word, weight in high_indicators.items() if word in content)\n",
    "        moderate_score = sum(weight for word, weight in moderate_indicators.items() if word in content)\n",
    "        \n",
    "        # Consider entity confidence scores\n",
    "        entity_severity = 0\n",
    "        for entities in medical_entities.values():\n",
    "            for entity in entities:\n",
    "                if any(word in entity.content.lower() for word in high_indicators.keys()):\n",
    "                    entity_severity += entity.confidence * 2\n",
    "                elif any(word in entity.content.lower() for word in moderate_indicators.keys()):\n",
    "                    entity_severity += entity.confidence\n",
    "        \n",
    "        total_score = high_score + entity_severity\n",
    "        \n",
    "        if total_score >= 5:\n",
    "            return \"High\"\n",
    "        elif total_score >= 2 or moderate_score >= 3:\n",
    "            return \"Moderate\"\n",
    "        else:\n",
    "            return \"Low\"\n",
    "    \n",
    "    def _calculate_prevalence(self, statistics: Dict[str, List[str]], disease_name: str) -> float:\n",
    "        \"\"\"Calculate disease prevalence using multiple data sources\"\"\"\n",
    "        # Try to extract from percentage statistics first\n",
    "        if 'percentage' in statistics:\n",
    "            try:\n",
    "                # Find the most reasonable prevalence percentage\n",
    "                percentages = [float(p.replace('%', '')) for p in statistics['percentage']]\n",
    "                # Filter out unrealistic values\n",
    "                reasonable_percentages = [p for p in percentages if 0.001 <= p <= 50]\n",
    "                if reasonable_percentages:\n",
    "                    return reasonable_percentages[0] / 100\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Try to calculate from affected population\n",
    "        if 'affected' in statistics:\n",
    "            try:\n",
    "                affected = statistics['affected'][0]\n",
    "                # Simple estimation based on global population\n",
    "                if 'million' in affected.lower():\n",
    "                    millions = float(re.search(r'(\\d+(?:\\.\\d+)?)', affected).group(1))\n",
    "                    return millions / 8000  # Rough global population\n",
    "            except (ValueError, AttributeError):\n",
    "                pass\n",
    "        \n",
    "        # Disease-specific default estimates based on medical knowledge\n",
    "        disease_lower = disease_name.lower()\n",
    "        default_prevalences = {\n",
    "            'diabetes': 0.11, 'hypertension': 0.22, 'depression': 0.05,\n",
    "            'cancer': 0.03, 'tuberculosis': 0.01, 'hiv': 0.01,\n",
    "            'hepatitis': 0.02, 'covid': 0.073\n",
    "        }\n",
    "        \n",
    "        for disease, prevalence in default_prevalences.items():\n",
    "            if disease in disease_lower:\n",
    "                return prevalence\n",
    "        \n",
    "        return 0.05  # Default 5% prevalence\n",
    "    \n",
    "    def _extract_key_facts(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"Extract key facts with improved detection\"\"\"\n",
    "        key_facts = []\n",
    "        \n",
    "        # Look for explicit key facts sections\n",
    "        key_facts_section = soup.find(['h2', 'h3'], string=re.compile(r'key facts?', re.I))\n",
    "        if key_facts_section:\n",
    "            # Get the list following this heading\n",
    "            next_element = key_facts_section.find_next(['ul', 'ol'])\n",
    "            if next_element:\n",
    "                for li in next_element.find_all('li'):\n",
    "                    fact = li.get_text().strip()\n",
    "                    if fact and len(fact) > 10:\n",
    "                        key_facts.append(fact)\n",
    "        \n",
    "        # If no explicit section, extract from first lists\n",
    "        if not key_facts:\n",
    "            lists = soup.find_all(['ul', 'ol'])[:2]  # First 2 lists\n",
    "            for ul in lists:\n",
    "                for li in ul.find_all('li')[:5]:  # Max 5 items per list\n",
    "                    fact = li.get_text().strip()\n",
    "                    if fact and len(fact) > 20:  # Substantial facts only\n",
    "                        key_facts.append(fact)\n",
    "        \n",
    "        return key_facts[:8]  # Limit to 8 key facts\n",
    "    \n",
    "    def _calculate_quality_score(self, medical_entities: Dict[str, List[MedicalEntity]], \n",
    "                                statistics: Dict[str, List[str]], overview: str, \n",
    "                                sections: Dict[str, str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate overall data quality score.\n",
    "        This is like a medical expert reviewing the completeness and\n",
    "        reliability of the extracted information.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        max_score = 10.0\n",
    "        \n",
    "        # Content completeness (40% of score)\n",
    "        required_sections = ['symptoms', 'treatment', 'diagnosis']\n",
    "        found_sections = sum(1 for section in required_sections if medical_entities.get(section))\n",
    "        score += (found_sections / len(required_sections)) * 4.0\n",
    "        \n",
    "        # Entity confidence (30% of score)\n",
    "        if medical_entities:\n",
    "            all_entities = [entity for entities in medical_entities.values() for entity in entities]\n",
    "            if all_entities:\n",
    "                avg_confidence = sum(entity.confidence for entity in all_entities) / len(all_entities)\n",
    "                score += avg_confidence * 3.0\n",
    "        \n",
    "        # Statistics presence (15% of score)\n",
    "        if statistics:\n",
    "            score += min(len(statistics) / 3.0, 1.0) * 1.5\n",
    "        \n",
    "        # Overview quality (10% of score)\n",
    "        if overview and len(overview) > 50:\n",
    "            score += 1.0\n",
    "        \n",
    "        # Section diversity (5% of score)\n",
    "        if len(sections) >= 5:\n",
    "            score += 0.5\n",
    "        \n",
    "        return min(score / max_score, 1.0)\n",
    "\n",
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating the enhanced scraper.\n",
    "    This orchestrates the entire process like a research project manager.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load configuration\n",
    "    config = ScrapingConfig(\n",
    "        max_concurrent_requests=3,  # Be respectful to WHO servers\n",
    "        request_delay=2.0,\n",
    "        enable_nlp=NLTK_AVAILABLE,\n",
    "        max_retries=3\n",
    "    )\n",
    "    \n",
    "    # Read URLs from file\n",
    "    try:\n",
    "        urls_file = Path('link-disease.txt')\n",
    "        if not urls_file.exists():\n",
    "            logger.error(\"link-disease.txt not found\")\n",
    "            return\n",
    "        \n",
    "        urls = []\n",
    "        with open(urls_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and line.startswith('http'):\n",
    "                    urls.append(line)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(urls)} URLs for processing\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading URLs: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize and run scraper\n",
    "    async with EnhancedWHOScraper(config) as scraper:\n",
    "        logger.info(\"Starting enhanced medical knowledge extraction...\")\n",
    "        \n",
    "        results = await scraper.scrape_urls(urls)\n",
    "        \n",
    "        logger.info(f\"Successfully processed {len(results)} diseases\")\n",
    "        \n",
    "        # Generate summary report\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ENHANCED MEDICAL KNOWLEDGE EXTRACTION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for disease_name, disease_info in results.items():\n",
    "            print(f\"\\nDisease: {disease_name}\")\n",
    "            print(f\"Quality Score: {disease_info.data_quality_score:.2f}\")\n",
    "            print(f\"Severity: {disease_info.severity}\")\n",
    "            print(f\"Prevalence: {disease_info.prevalence:.3f}\")\n",
    "            print(f\"Entities Extracted:\")\n",
    "            print(f\"  - Symptoms: {len(disease_info.symptoms)}\")\n",
    "            print(f\"  - Risk Factors: {len(disease_info.risk_factors)}\")\n",
    "            print(f\"  - Prevention: {len(disease_info.prevention)}\")\n",
    "            print(f\"  - Diagnosis: {len(disease_info.diagnosis)}\")\n",
    "            print(f\"  - Treatment: {len(disease_info.treatment)}\")\n",
    "            \n",
    "            if disease_info.symptoms:\n",
    "                print(f\"  Top Symptom: {disease_info.symptoms[0].content[:100]}...\")\n",
    "        \n",
    "        print(f\"\\nData saved to medical_knowledge.db\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-chatbot-main-DR3YVifn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
